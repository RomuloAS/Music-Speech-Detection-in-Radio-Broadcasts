{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2: Optimisation, Pre-/Post-processing, Cost-sensitive Learning, Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing \n",
    "\n",
    "* [loadarff (scify)][loadarff]: Read an arff file.\n",
    "* [glob][glob]: The glob module finds all the pathnames matching a specified pattern.\n",
    "* [pandas][pandas]: Pandas stands for â€œPython Data Analysis Library\".\n",
    "* [warnings][warnings]: It filters warnings messages, and ignores them.\n",
    "\n",
    "[loadarff]:https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.arff.loadarff.html\n",
    "[glob]:https://docs.python.org/3/library/glob.html\n",
    "[pandas]:http://pandas.pydata.org/pandas-docs/stable/\n",
    "[warnings]:https://docs.python.org/3/library/warnings.html#warnings.filterwarnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Loading\n",
    "\n",
    "Read all files inside \"Train\" folder, and sort list of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = glob(\"../Train/*.arff\", recursive=True)\n",
    "ff = sorted(ff, key=lambda x: int(''.join(filter(str.isdigit, x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select speech files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_files = []\n",
    "\n",
    "for f in ff:\n",
    "    if \"speech\" in f:\n",
    "        speech_files.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load arff files to pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for f in speech_files: \n",
    "    with open(f) as arff:\n",
    "        data, meta = loadarff(arff)\n",
    "        dff = pd.DataFrame(data)\n",
    "        df = pd.concat([df, dff], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing class\n",
    "\n",
    "Remove class column from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing \n",
    "\n",
    "* [matplotlib pyplot][pyplot]: A collection of command style functions that make matplotlib work like MATLAB.\n",
    "* [matplotlib cm][cm]: Builtin colormaps, colormap handling utilities, and the ScalarMappable mixin.\n",
    "\n",
    "[pyplot]:https://matplotlib.org/3.1.0/tutorials/introductory/pyplot.html\n",
    "[cm]:https://matplotlib.org/3.1.0/api/cm_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting heatmap\n",
    "\n",
    "Plot heatmap from correlation (pearson is default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cor = ax.matshow(df.corr(), cmap=cm.gray)\n",
    "fig.colorbar(cor)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing \n",
    "\n",
    "* [scikit IsolationForest][IsolationForest]: Return the anomaly score of each sample using the IsolationForest algorithm.\n",
    "\n",
    "[IsolationForest]:https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting outliers\n",
    "\n",
    "Detect outliers in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = IsolationForest(behaviour = 'new', contamination= 'auto', n_jobs=8)\n",
    "preds = clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing outliers\n",
    "\n",
    "Print all outliers in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f000001</th>\n",
       "      <th>f000002</th>\n",
       "      <th>f000003</th>\n",
       "      <th>f000004</th>\n",
       "      <th>f000005</th>\n",
       "      <th>f000006</th>\n",
       "      <th>f000007</th>\n",
       "      <th>f000008</th>\n",
       "      <th>f000009</th>\n",
       "      <th>f000010</th>\n",
       "      <th>...</th>\n",
       "      <th>f000095</th>\n",
       "      <th>f000096</th>\n",
       "      <th>f000097</th>\n",
       "      <th>f000098</th>\n",
       "      <th>f000099</th>\n",
       "      <th>f000100</th>\n",
       "      <th>f000101</th>\n",
       "      <th>f000102</th>\n",
       "      <th>f000103</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071211</td>\n",
       "      <td>0.196920</td>\n",
       "      <td>0.249267</td>\n",
       "      <td>0.182908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.039364</td>\n",
       "      <td>-0.202094</td>\n",
       "      <td>-0.048832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048674</td>\n",
       "      <td>0.325177</td>\n",
       "      <td>0.137358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.069519</td>\n",
       "      <td>-0.206843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112350</td>\n",
       "      <td>0.316342</td>\n",
       "      <td>0.342796</td>\n",
       "      <td>0.241414</td>\n",
       "      <td>0.110744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195092</td>\n",
       "      <td>0.241459</td>\n",
       "      <td>0.045340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033996</td>\n",
       "      <td>0.177675</td>\n",
       "      <td>0.318122</td>\n",
       "      <td>0.293416</td>\n",
       "      <td>0.141933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.026712</td>\n",
       "      <td>-0.030398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073773</td>\n",
       "      <td>0.158904</td>\n",
       "      <td>0.106583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.106583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.090301</td>\n",
       "      <td>-0.274135</td>\n",
       "      <td>-0.308518</td>\n",
       "      <td>-0.090114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163330</td>\n",
       "      <td>0.318413</td>\n",
       "      <td>0.397841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031367</td>\n",
       "      <td>0.298903</td>\n",
       "      <td>0.389898</td>\n",
       "      <td>0.262742</td>\n",
       "      <td>0.183866</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>0.056638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.117856</td>\n",
       "      <td>-0.237919</td>\n",
       "      <td>-0.147653</td>\n",
       "      <td>-0.098759</td>\n",
       "      <td>-0.117262</td>\n",
       "      <td>-0.148298</td>\n",
       "      <td>-0.021723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>-0.098831</td>\n",
       "      <td>-0.049586</td>\n",
       "      <td>-0.006987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.116004</td>\n",
       "      <td>-0.197139</td>\n",
       "      <td>-0.260605</td>\n",
       "      <td>-0.258532</td>\n",
       "      <td>-0.302422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.043047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.166577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.166577</td>\n",
       "      <td>-0.315078</td>\n",
       "      <td>-0.319015</td>\n",
       "      <td>-0.293458</td>\n",
       "      <td>-0.239871</td>\n",
       "      <td>-0.079997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007911</td>\n",
       "      <td>0.202148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.202148</td>\n",
       "      <td>0.337331</td>\n",
       "      <td>0.419488</td>\n",
       "      <td>0.400123</td>\n",
       "      <td>0.335542</td>\n",
       "      <td>0.318250</td>\n",
       "      <td>0.367358</td>\n",
       "      <td>0.401356</td>\n",
       "      <td>0.347824</td>\n",
       "      <td>0.153033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.107257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054077</td>\n",
       "      <td>0.110654</td>\n",
       "      <td>0.087296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027994</td>\n",
       "      <td>0.120829</td>\n",
       "      <td>0.231255</td>\n",
       "      <td>0.334455</td>\n",
       "      <td>0.245290</td>\n",
       "      <td>0.198968</td>\n",
       "      <td>0.110238</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144666</td>\n",
       "      <td>0.182517</td>\n",
       "      <td>0.031127</td>\n",
       "      <td>0.028441</td>\n",
       "      <td>0.093953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>b'no_speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>0.093953</td>\n",
       "      <td>0.128505</td>\n",
       "      <td>0.061709</td>\n",
       "      <td>0.040294</td>\n",
       "      <td>0.115160</td>\n",
       "      <td>0.209808</td>\n",
       "      <td>0.346175</td>\n",
       "      <td>0.252415</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>-0.146110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>b'no_speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>b'no_speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.056576</td>\n",
       "      <td>-0.274909</td>\n",
       "      <td>-0.320361</td>\n",
       "      <td>-0.312861</td>\n",
       "      <td>-0.318182</td>\n",
       "      <td>-0.243398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>b'no_speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>b'no_speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.029341</td>\n",
       "      <td>0.098610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.038977</td>\n",
       "      <td>-0.148681</td>\n",
       "      <td>-0.296905</td>\n",
       "      <td>-0.373182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>b'no_speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3242</th>\n",
       "      <td>-0.296905</td>\n",
       "      <td>-0.373182</td>\n",
       "      <td>-0.421019</td>\n",
       "      <td>-0.408458</td>\n",
       "      <td>-0.375734</td>\n",
       "      <td>-0.332893</td>\n",
       "      <td>-0.295388</td>\n",
       "      <td>-0.306418</td>\n",
       "      <td>-0.154071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>b'no_speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>0.149554</td>\n",
       "      <td>0.122041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.050714</td>\n",
       "      <td>0.158035</td>\n",
       "      <td>-0.096408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>0.158035</td>\n",
       "      <td>-0.096408</td>\n",
       "      <td>-0.256841</td>\n",
       "      <td>-0.137702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016135</td>\n",
       "      <td>0.138690</td>\n",
       "      <td>0.088005</td>\n",
       "      <td>-0.080749</td>\n",
       "      <td>-0.107118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>-0.107118</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.237337</td>\n",
       "      <td>-0.214682</td>\n",
       "      <td>-0.021029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.134339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>-0.134339</td>\n",
       "      <td>-0.125568</td>\n",
       "      <td>-0.086151</td>\n",
       "      <td>-0.143401</td>\n",
       "      <td>-0.176671</td>\n",
       "      <td>-0.308097</td>\n",
       "      <td>-0.443090</td>\n",
       "      <td>-0.506017</td>\n",
       "      <td>-0.190311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>-0.190311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251221</td>\n",
       "      <td>0.432782</td>\n",
       "      <td>0.326425</td>\n",
       "      <td>0.147830</td>\n",
       "      <td>0.092167</td>\n",
       "      <td>0.128642</td>\n",
       "      <td>0.189310</td>\n",
       "      <td>0.334161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'no_speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250250</th>\n",
       "      <td>0.157365</td>\n",
       "      <td>0.185094</td>\n",
       "      <td>0.133661</td>\n",
       "      <td>0.062407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.090478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250251</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196563</td>\n",
       "      <td>0.416948</td>\n",
       "      <td>0.185628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250253</th>\n",
       "      <td>-0.019078</td>\n",
       "      <td>-0.152744</td>\n",
       "      <td>-0.194943</td>\n",
       "      <td>-0.211395</td>\n",
       "      <td>-0.049415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071114</td>\n",
       "      <td>0.176060</td>\n",
       "      <td>0.047516</td>\n",
       "      <td>-0.050314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250254</th>\n",
       "      <td>-0.050314</td>\n",
       "      <td>0.021628</td>\n",
       "      <td>0.074767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.228165</td>\n",
       "      <td>-0.318697</td>\n",
       "      <td>-0.327938</td>\n",
       "      <td>-0.311444</td>\n",
       "      <td>-0.311323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250257</th>\n",
       "      <td>0.230892</td>\n",
       "      <td>0.283550</td>\n",
       "      <td>0.175382</td>\n",
       "      <td>0.019686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039507</td>\n",
       "      <td>-0.319652</td>\n",
       "      <td>-0.296836</td>\n",
       "      <td>0.110568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250258</th>\n",
       "      <td>0.110568</td>\n",
       "      <td>0.268501</td>\n",
       "      <td>0.372267</td>\n",
       "      <td>0.406619</td>\n",
       "      <td>0.366093</td>\n",
       "      <td>0.149927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.146554</td>\n",
       "      <td>-0.171740</td>\n",
       "      <td>-0.111760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250259</th>\n",
       "      <td>-0.171740</td>\n",
       "      <td>-0.111760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.079076</td>\n",
       "      <td>-0.034383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.025674</td>\n",
       "      <td>-0.067987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250260</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250261</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049834</td>\n",
       "      <td>0.237043</td>\n",
       "      <td>0.376483</td>\n",
       "      <td>0.354284</td>\n",
       "      <td>0.357645</td>\n",
       "      <td>0.118049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.321438</td>\n",
       "      <td>-0.310428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250265</th>\n",
       "      <td>0.179422</td>\n",
       "      <td>0.105656</td>\n",
       "      <td>0.068034</td>\n",
       "      <td>0.053839</td>\n",
       "      <td>0.053475</td>\n",
       "      <td>-0.142031</td>\n",
       "      <td>-0.313042</td>\n",
       "      <td>-0.241110</td>\n",
       "      <td>0.011849</td>\n",
       "      <td>0.058375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250267</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070189</td>\n",
       "      <td>-0.133499</td>\n",
       "      <td>-0.172787</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.208714</td>\n",
       "      <td>0.230839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250268</th>\n",
       "      <td>0.230839</td>\n",
       "      <td>0.249878</td>\n",
       "      <td>0.283817</td>\n",
       "      <td>0.148130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003199</td>\n",
       "      <td>-0.199305</td>\n",
       "      <td>-0.334927</td>\n",
       "      <td>-0.098292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250269</th>\n",
       "      <td>-0.334927</td>\n",
       "      <td>-0.098292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085685</td>\n",
       "      <td>0.163157</td>\n",
       "      <td>0.080680</td>\n",
       "      <td>0.046464</td>\n",
       "      <td>0.072458</td>\n",
       "      <td>0.147923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250270</th>\n",
       "      <td>0.147923</td>\n",
       "      <td>0.199299</td>\n",
       "      <td>0.116069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250271</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008985</td>\n",
       "      <td>0.222218</td>\n",
       "      <td>0.115212</td>\n",
       "      <td>-0.222553</td>\n",
       "      <td>-0.165548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.050446</td>\n",
       "      <td>-0.178707</td>\n",
       "      <td>-0.158865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250277</th>\n",
       "      <td>0.079008</td>\n",
       "      <td>0.075228</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.005342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032583</td>\n",
       "      <td>0.108030</td>\n",
       "      <td>0.192276</td>\n",
       "      <td>0.208453</td>\n",
       "      <td>0.205473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250291</th>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.053167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250292</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043198</td>\n",
       "      <td>0.273178</td>\n",
       "      <td>0.221599</td>\n",
       "      <td>0.075067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250293</th>\n",
       "      <td>0.099655</td>\n",
       "      <td>0.099176</td>\n",
       "      <td>0.014430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.038367</td>\n",
       "      <td>-0.070229</td>\n",
       "      <td>-0.208790</td>\n",
       "      <td>-0.277760</td>\n",
       "      <td>-0.198541</td>\n",
       "      <td>-0.078228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250294</th>\n",
       "      <td>-0.078228</td>\n",
       "      <td>-0.016027</td>\n",
       "      <td>-0.090940</td>\n",
       "      <td>0.058270</td>\n",
       "      <td>0.096799</td>\n",
       "      <td>-0.022531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.038240</td>\n",
       "      <td>-0.076836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250295</th>\n",
       "      <td>-0.038240</td>\n",
       "      <td>-0.076836</td>\n",
       "      <td>-0.012115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>-0.124675</td>\n",
       "      <td>-0.355754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250296</th>\n",
       "      <td>-0.355754</td>\n",
       "      <td>-0.320247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121919</td>\n",
       "      <td>0.205173</td>\n",
       "      <td>0.171549</td>\n",
       "      <td>0.089999</td>\n",
       "      <td>0.050110</td>\n",
       "      <td>-0.257807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250297</th>\n",
       "      <td>0.050110</td>\n",
       "      <td>-0.257807</td>\n",
       "      <td>-0.466555</td>\n",
       "      <td>-0.255848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.146986</td>\n",
       "      <td>-0.420293</td>\n",
       "      <td>-0.508102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250298</th>\n",
       "      <td>-0.508102</td>\n",
       "      <td>-0.393479</td>\n",
       "      <td>-0.164237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040206</td>\n",
       "      <td>0.194709</td>\n",
       "      <td>0.125542</td>\n",
       "      <td>0.030868</td>\n",
       "      <td>0.058465</td>\n",
       "      <td>0.265492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250299</th>\n",
       "      <td>0.265492</td>\n",
       "      <td>0.355532</td>\n",
       "      <td>0.211242</td>\n",
       "      <td>0.189314</td>\n",
       "      <td>0.257538</td>\n",
       "      <td>0.259424</td>\n",
       "      <td>0.178811</td>\n",
       "      <td>0.235469</td>\n",
       "      <td>0.346152</td>\n",
       "      <td>0.367731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250308</th>\n",
       "      <td>0.006550</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.246469</td>\n",
       "      <td>-0.240334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100186</td>\n",
       "      <td>0.165415</td>\n",
       "      <td>0.133125</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250310</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.042800</td>\n",
       "      <td>-0.116266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022374</td>\n",
       "      <td>0.168941</td>\n",
       "      <td>0.277523</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>0.011103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250319</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212451</td>\n",
       "      <td>0.360107</td>\n",
       "      <td>0.043665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250321</th>\n",
       "      <td>0.010870</td>\n",
       "      <td>-0.189738</td>\n",
       "      <td>-0.449661</td>\n",
       "      <td>-0.329223</td>\n",
       "      <td>0.026215</td>\n",
       "      <td>0.157382</td>\n",
       "      <td>0.307632</td>\n",
       "      <td>0.329981</td>\n",
       "      <td>0.228578</td>\n",
       "      <td>0.143872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250327</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069055</td>\n",
       "      <td>0.039808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>0.151414</td>\n",
       "      <td>0.245569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>b'speech'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12854 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f000001   f000002   f000003   f000004   f000005   f000006   f000007  \\\n",
       "70      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.071211   \n",
       "114     0.000000 -0.039364 -0.202094 -0.048832  0.000000  0.000000  0.000000   \n",
       "150     0.000000  0.000000  0.000000  0.048674  0.325177  0.137358  0.000000   \n",
       "151     0.000000  0.000000  0.112350  0.316342  0.342796  0.241414  0.110744   \n",
       "152     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.195092   \n",
       "153     0.000000  0.000000  0.000000  0.033996  0.177675  0.318122  0.293416   \n",
       "154     0.000000  0.000000  0.000000 -0.026712 -0.030398  0.000000  0.073773   \n",
       "155     0.106583  0.000000 -0.090301 -0.274135 -0.308518 -0.090114  0.000000   \n",
       "196     0.000000  0.000000  0.031367  0.298903  0.389898  0.262742  0.183866   \n",
       "197     0.000000 -0.117856 -0.237919 -0.147653 -0.098759 -0.117262 -0.148298   \n",
       "311    -0.098831 -0.049586 -0.006987  0.000000  0.000000 -0.116004 -0.197139   \n",
       "583     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "593     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "594     0.000000 -0.166577 -0.315078 -0.319015 -0.293458 -0.239871 -0.079997   \n",
       "595     0.202148  0.337331  0.419488  0.400123  0.335542  0.318250  0.367358   \n",
       "772     0.000000 -0.107257  0.000000  0.000000  0.054077  0.110654  0.087296   \n",
       "773     0.000000  0.027994  0.120829  0.231255  0.334455  0.245290  0.198968   \n",
       "1605    0.000000  0.000000  0.000000  0.000000  0.000000  0.144666  0.182517   \n",
       "1606    0.093953  0.128505  0.061709  0.040294  0.115160  0.209808  0.346175   \n",
       "1608    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1609    0.000000  0.000000  0.000000  0.000000 -0.056576 -0.274909 -0.320361   \n",
       "3240    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3241    0.000000  0.000000  0.000000 -0.029341  0.098610  0.000000 -0.038977   \n",
       "3242   -0.296905 -0.373182 -0.421019 -0.408458 -0.375734 -0.332893 -0.295388   \n",
       "5196    0.149554  0.122041  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5197    0.158035 -0.096408 -0.256841 -0.137702  0.000000  0.016135  0.138690   \n",
       "5198   -0.107118  0.006542  0.000000  0.000000  0.000000 -0.237337 -0.214682   \n",
       "5199   -0.134339 -0.125568 -0.086151 -0.143401 -0.176671 -0.308097 -0.443090   \n",
       "5200   -0.190311  0.000000  0.251221  0.432782  0.326425  0.147830  0.092167   \n",
       "5568    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "250250  0.157365  0.185094  0.133661  0.062407  0.000000  0.000000  0.000000   \n",
       "250251  0.000000  0.000000  0.196563  0.416948  0.185628  0.000000  0.000000   \n",
       "250253 -0.019078 -0.152744 -0.194943 -0.211395 -0.049415  0.000000  0.071114   \n",
       "250254 -0.050314  0.021628  0.074767  0.000000  0.000000 -0.228165 -0.318697   \n",
       "250257  0.230892  0.283550  0.175382  0.019686  0.000000  0.000000  0.039507   \n",
       "250258  0.110568  0.268501  0.372267  0.406619  0.366093  0.149927  0.000000   \n",
       "250259 -0.171740 -0.111760  0.000000 -0.079076 -0.034383  0.000000 -0.025674   \n",
       "250260  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "250261  0.000000  0.049834  0.237043  0.376483  0.354284  0.357645  0.118049   \n",
       "250265  0.179422  0.105656  0.068034  0.053839  0.053475 -0.142031 -0.313042   \n",
       "250267  0.000000  0.000000  0.000000  0.070189 -0.133499 -0.172787  0.027653   \n",
       "250268  0.230839  0.249878  0.283817  0.148130  0.000000  0.000000 -0.003199   \n",
       "250269 -0.334927 -0.098292  0.000000  0.000000  0.085685  0.163157  0.080680   \n",
       "250270  0.147923  0.199299  0.116069  0.000000  0.000000  0.000000  0.000000   \n",
       "250271  0.000000  0.008985  0.222218  0.115212 -0.222553 -0.165548  0.000000   \n",
       "250277  0.079008  0.075228 -0.022529 -0.005342  0.000000  0.032583  0.108030   \n",
       "250291  0.000583  0.000000  0.000000  0.000500  0.053167  0.000000  0.000000   \n",
       "250292  0.000000  0.043198  0.273178  0.221599  0.075067  0.000000  0.000000   \n",
       "250293  0.099655  0.099176  0.014430  0.000000 -0.038367 -0.070229 -0.208790   \n",
       "250294 -0.078228 -0.016027 -0.090940  0.058270  0.096799 -0.022531  0.000000   \n",
       "250295 -0.038240 -0.076836 -0.012115  0.000000  0.000000  0.000000  0.000000   \n",
       "250296 -0.355754 -0.320247  0.000000  0.000000  0.121919  0.205173  0.171549   \n",
       "250297  0.050110 -0.257807 -0.466555 -0.255848  0.000000  0.000000  0.000000   \n",
       "250298 -0.508102 -0.393479 -0.164237  0.000000  0.040206  0.194709  0.125542   \n",
       "250299  0.265492  0.355532  0.211242  0.189314  0.257538  0.259424  0.178811   \n",
       "250308  0.006550 -0.000001 -0.246469 -0.240334  0.000000  0.100186  0.165415   \n",
       "250310  0.000000 -0.042800 -0.116266  0.000000  0.022374  0.168941  0.277523   \n",
       "250319  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "250321  0.010870 -0.189738 -0.449661 -0.329223  0.026215  0.157382  0.307632   \n",
       "250327  0.000000  0.000000  0.069055  0.039808  0.000000  0.000000  0.000000   \n",
       "\n",
       "         f000008   f000009   f000010      ...        f000095   f000096  \\\n",
       "70      0.196920  0.249267  0.182908      ...       0.085106  0.085106   \n",
       "114     0.000000  0.000000  0.020792      ...       0.127660  0.042553   \n",
       "150    -0.069519 -0.206843  0.000000      ...       0.042553  0.212766   \n",
       "151     0.000000  0.000000  0.000000      ...       0.085106  0.234043   \n",
       "152     0.241459  0.045340  0.000000      ...       0.085106  0.170213   \n",
       "153     0.141933  0.000000  0.000000      ...       0.042553  0.127660   \n",
       "154     0.158904  0.106583  0.000000      ...       0.042553  0.085106   \n",
       "155     0.163330  0.318413  0.397841      ...       0.042553  0.085106   \n",
       "196     0.139346  0.056638  0.000000      ...       0.085106  0.000000   \n",
       "197    -0.021723  0.000000  0.014359      ...       0.085106  0.000000   \n",
       "311    -0.260605 -0.258532 -0.302422      ...       0.085106  0.085106   \n",
       "583     0.000000  0.000000 -0.043047      ...       0.000000  0.063830   \n",
       "593     0.000000  0.000000 -0.166577      ...       0.042553  0.042553   \n",
       "594     0.000000  0.007911  0.202148      ...       0.042553  0.042553   \n",
       "595     0.401356  0.347824  0.153033      ...       0.042553  0.085106   \n",
       "772     0.000000  0.000000  0.027994      ...       0.085106  0.127660   \n",
       "773     0.110238  0.036893  0.000000      ...       0.085106  0.085106   \n",
       "1605    0.031127  0.028441  0.093953      ...       0.042553  0.042553   \n",
       "1606    0.252415  0.007487 -0.146110      ...       0.042553  0.042553   \n",
       "1608    0.000000  0.000000  0.000000      ...       0.042553  0.085106   \n",
       "1609   -0.312861 -0.318182 -0.243398      ...       0.042553  0.085106   \n",
       "3240    0.000000  0.000000  0.000000      ...       0.042553  0.042553   \n",
       "3241   -0.148681 -0.296905 -0.373182      ...       0.042553  0.042553   \n",
       "3242   -0.306418 -0.154071  0.000000      ...       0.042553  0.042553   \n",
       "5196   -0.050714  0.158035 -0.096408      ...       0.042553  0.000000   \n",
       "5197    0.088005 -0.080749 -0.107118      ...       0.085106  0.000000   \n",
       "5198   -0.021029  0.000000 -0.134339      ...       0.085106  0.000000   \n",
       "5199   -0.506017 -0.190311  0.000000      ...       0.063830  0.000000   \n",
       "5200    0.128642  0.189310  0.334161      ...       0.042553  0.000000   \n",
       "5568    0.000000  0.000000  0.000000      ...       0.085106  0.042553   \n",
       "...          ...       ...       ...      ...            ...       ...   \n",
       "250250 -0.090478  0.000000  0.000000      ...       0.063830  0.127660   \n",
       "250251  0.000000  0.000000  0.019114      ...       0.042553  0.085106   \n",
       "250253  0.176060  0.047516 -0.050314      ...       0.085106  0.085106   \n",
       "250254 -0.327938 -0.311444 -0.311323      ...       0.085106  0.085106   \n",
       "250257 -0.319652 -0.296836  0.110568      ...       0.042553  0.042553   \n",
       "250258 -0.146554 -0.171740 -0.111760      ...       0.063830  0.063830   \n",
       "250259 -0.067987  0.000000  0.000000      ...       0.085106  0.085106   \n",
       "250260  0.000000  0.000000  0.049834      ...       0.085106  0.127660   \n",
       "250261  0.000000 -0.321438 -0.310428      ...       0.085106  0.127660   \n",
       "250265 -0.241110  0.011849  0.058375      ...       0.170213  0.170213   \n",
       "250267  0.135417  0.208714  0.230839      ...       0.085106  0.170213   \n",
       "250268 -0.199305 -0.334927 -0.098292      ...       0.042553  0.170213   \n",
       "250269  0.046464  0.072458  0.147923      ...       0.021277  0.148936   \n",
       "250270  0.000000  0.000000  0.000000      ...       0.042553  0.170213   \n",
       "250271 -0.050446 -0.178707 -0.158865      ...       0.042553  0.148936   \n",
       "250277  0.192276  0.208453  0.205473      ...       0.085106  0.085106   \n",
       "250291  0.000000  0.000000  0.043198      ...       0.127660  0.085106   \n",
       "250292  0.000000  0.000000  0.099655      ...       0.127660  0.085106   \n",
       "250293 -0.277760 -0.198541 -0.078228      ...       0.085106  0.085106   \n",
       "250294 -0.000089 -0.038240 -0.076836      ...       0.042553  0.042553   \n",
       "250295  0.021739 -0.124675 -0.355754      ...       0.127660  0.063830   \n",
       "250296  0.089999  0.050110 -0.257807      ...       0.127660  0.085106   \n",
       "250297 -0.146986 -0.420293 -0.508102      ...       0.127660  0.085106   \n",
       "250298  0.030868  0.058465  0.265492      ...       0.127660  0.085106   \n",
       "250299  0.235469  0.346152  0.367731      ...       0.127660  0.085106   \n",
       "250308  0.133125  0.002524  0.000000      ...       0.085106  0.085106   \n",
       "250310  0.177449  0.011103  0.000000      ...       0.021277  0.063830   \n",
       "250319  0.212451  0.360107  0.043665      ...       0.042553  0.000000   \n",
       "250321  0.329981  0.228578  0.143872      ...       0.000000  0.042553   \n",
       "250327  0.020675  0.151414  0.245569      ...       0.000000  0.085106   \n",
       "\n",
       "         f000097   f000098   f000099   f000100   f000101   f000102   f000103  \\\n",
       "70      0.170213  0.085106  0.000000  0.042553  0.000000  0.000000  0.000000   \n",
       "114     0.085106  0.042553  0.000000  0.000000  0.042553  0.127660  0.000000   \n",
       "150     0.148936  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "151     0.127660  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "152     0.127660  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "153     0.106383  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "154     0.127660  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "155     0.085106  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "196     0.042553  0.042553  0.000000  0.000000  0.000000  0.042553  0.000000   \n",
       "197     0.042553  0.042553  0.000000  0.000000  0.000000  0.042553  0.000000   \n",
       "311     0.106383  0.000000  0.042553  0.042553  0.042553  0.042553  0.042553   \n",
       "583     0.042553  0.000000  0.000000  0.042553  0.085106  0.042553  0.000000   \n",
       "593     0.000000  0.042553  0.000000  0.000000  0.042553  0.085106  0.085106   \n",
       "594     0.000000  0.042553  0.000000  0.000000  0.042553  0.085106  0.042553   \n",
       "595     0.000000  0.042553  0.000000  0.000000  0.000000  0.063830  0.042553   \n",
       "772     0.042553  0.127660  0.042553  0.000000  0.000000  0.000000  0.000000   \n",
       "773     0.021277  0.127660  0.042553  0.000000  0.000000  0.000000  0.000000   \n",
       "1605    0.085106  0.042553  0.085106  0.042553  0.042553  0.042553  0.085106   \n",
       "1606    0.042553  0.000000  0.042553  0.000000  0.042553  0.042553  0.085106   \n",
       "1608    0.042553  0.000000  0.042553  0.000000  0.042553  0.085106  0.127660   \n",
       "1609    0.042553  0.000000  0.042553  0.000000  0.042553  0.085106  0.127660   \n",
       "3240    0.000000  0.000000  0.000000  0.000000  0.042553  0.042553  0.085106   \n",
       "3241    0.000000  0.000000  0.000000  0.000000  0.042553  0.042553  0.085106   \n",
       "3242    0.000000  0.000000  0.000000  0.000000  0.042553  0.042553  0.085106   \n",
       "5196    0.085106  0.042553  0.000000  0.000000  0.085106  0.000000  0.000000   \n",
       "5197    0.042553  0.085106  0.000000  0.000000  0.085106  0.000000  0.000000   \n",
       "5198    0.042553  0.127660  0.000000  0.021277  0.042553  0.000000  0.063830   \n",
       "5199    0.021277  0.127660  0.000000  0.042553  0.021277  0.000000  0.085106   \n",
       "5200    0.000000  0.085106  0.000000  0.042553  0.000000  0.000000  0.085106   \n",
       "5568    0.000000  0.042553  0.000000  0.000000  0.042553  0.000000  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "250250  0.000000  0.063830  0.000000  0.000000  0.042553  0.021277  0.000000   \n",
       "250251  0.000000  0.000000  0.000000  0.000000  0.042553  0.042553  0.042553   \n",
       "250253  0.021277  0.042553  0.000000  0.000000  0.042553  0.042553  0.042553   \n",
       "250254  0.042553  0.042553  0.000000  0.000000  0.042553  0.042553  0.042553   \n",
       "250257  0.042553  0.000000  0.042553  0.042553  0.000000  0.042553  0.000000   \n",
       "250258  0.085106  0.000000  0.085106  0.042553  0.000000  0.042553  0.000000   \n",
       "250259  0.042553  0.000000  0.085106  0.042553  0.000000  0.042553  0.000000   \n",
       "250260  0.042553  0.042553  0.085106  0.085106  0.000000  0.042553  0.000000   \n",
       "250261  0.042553  0.042553  0.042553  0.085106  0.042553  0.042553  0.000000   \n",
       "250265  0.063830  0.127660  0.000000  0.085106  0.042553  0.063830  0.000000   \n",
       "250267  0.063830  0.170213  0.000000  0.042553  0.000000  0.042553  0.000000   \n",
       "250268  0.042553  0.127660  0.000000  0.063830  0.000000  0.042553  0.000000   \n",
       "250269  0.042553  0.085106  0.000000  0.042553  0.000000  0.063830  0.021277   \n",
       "250270  0.042553  0.042553  0.021277  0.042553  0.000000  0.127660  0.042553   \n",
       "250271  0.000000  0.042553  0.042553  0.042553  0.000000  0.085106  0.042553   \n",
       "250277  0.000000  0.042553  0.000000  0.042553  0.000000  0.170213  0.042553   \n",
       "250291  0.106383  0.191489  0.042553  0.127660  0.000000  0.170213  0.127660   \n",
       "250292  0.170213  0.170213  0.042553  0.127660  0.042553  0.212766  0.127660   \n",
       "250293  0.191489  0.170213  0.085106  0.042553  0.042553  0.170213  0.127660   \n",
       "250294  0.212766  0.170213  0.085106  0.021277  0.042553  0.127660  0.085106   \n",
       "250295  0.255319  0.170213  0.085106  0.042553  0.042553  0.127660  0.085106   \n",
       "250296  0.255319  0.085106  0.085106  0.085106  0.042553  0.063830  0.042553   \n",
       "250297  0.212766  0.085106  0.085106  0.085106  0.021277  0.063830  0.042553   \n",
       "250298  0.170213  0.085106  0.063830  0.085106  0.000000  0.042553  0.042553   \n",
       "250299  0.127660  0.085106  0.085106  0.085106  0.000000  0.042553  0.000000   \n",
       "250308  0.000000  0.000000  0.127660  0.042553  0.042553  0.042553  0.042553   \n",
       "250310  0.000000  0.000000  0.106383  0.000000  0.000000  0.000000  0.000000   \n",
       "250319  0.000000  0.000000  0.000000  0.042553  0.042553  0.085106  0.085106   \n",
       "250321  0.042553  0.000000  0.000000  0.042553  0.000000  0.127660  0.085106   \n",
       "250327  0.085106  0.085106  0.000000  0.042553  0.000000  0.085106  0.042553   \n",
       "\n",
       "               class  \n",
       "70         b'speech'  \n",
       "114        b'speech'  \n",
       "150        b'speech'  \n",
       "151        b'speech'  \n",
       "152        b'speech'  \n",
       "153        b'speech'  \n",
       "154        b'speech'  \n",
       "155        b'speech'  \n",
       "196        b'speech'  \n",
       "197        b'speech'  \n",
       "311        b'speech'  \n",
       "583        b'speech'  \n",
       "593        b'speech'  \n",
       "594        b'speech'  \n",
       "595        b'speech'  \n",
       "772        b'speech'  \n",
       "773        b'speech'  \n",
       "1605    b'no_speech'  \n",
       "1606    b'no_speech'  \n",
       "1608    b'no_speech'  \n",
       "1609    b'no_speech'  \n",
       "3240    b'no_speech'  \n",
       "3241    b'no_speech'  \n",
       "3242    b'no_speech'  \n",
       "5196       b'speech'  \n",
       "5197       b'speech'  \n",
       "5198       b'speech'  \n",
       "5199       b'speech'  \n",
       "5200       b'speech'  \n",
       "5568    b'no_speech'  \n",
       "...              ...  \n",
       "250250     b'speech'  \n",
       "250251     b'speech'  \n",
       "250253     b'speech'  \n",
       "250254     b'speech'  \n",
       "250257     b'speech'  \n",
       "250258     b'speech'  \n",
       "250259     b'speech'  \n",
       "250260     b'speech'  \n",
       "250261     b'speech'  \n",
       "250265     b'speech'  \n",
       "250267     b'speech'  \n",
       "250268     b'speech'  \n",
       "250269     b'speech'  \n",
       "250270     b'speech'  \n",
       "250271     b'speech'  \n",
       "250277     b'speech'  \n",
       "250291     b'speech'  \n",
       "250292     b'speech'  \n",
       "250293     b'speech'  \n",
       "250294     b'speech'  \n",
       "250295     b'speech'  \n",
       "250296     b'speech'  \n",
       "250297     b'speech'  \n",
       "250298     b'speech'  \n",
       "250299     b'speech'  \n",
       "250308     b'speech'  \n",
       "250310     b'speech'  \n",
       "250319     b'speech'  \n",
       "250321     b'speech'  \n",
       "250327     b'speech'  \n",
       "\n",
       "[12854 rows x 104 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[preds == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers\n",
    "\n",
    "Remove outliers from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape (251699, 104)\n",
      "Outliers shape (238845, 104)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original shape\", df.shape)\n",
    "df = df[preds != -1]\n",
    "print(\"Outliers shape\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing \n",
    "\n",
    "* [scikit LabelEncoder][LabelEncoder]: Encode labels with value between 0 and n_classes-1.\n",
    "\n",
    "[LabelEncoder]:https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing and Encoding class\n",
    "\n",
    "* Remove class column from the dataframe and creates variable X from it.\n",
    "* Encode class column and creates variable y from it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "X = df.loc[:, df.columns != 'class']\n",
    "y = le.fit_transform(df[\"class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing \n",
    "\n",
    "* [statsmodels sm][sm]: Functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.\n",
    "\n",
    "[sm]:http://www.statsmodels.org/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting features\n",
    "\n",
    "Feature selection using backward elimination with linear regression (ordinary least squares model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f000002', 'f000003', 'f000010', 'f000014', 'f000018', 'f000022', 'f000026', 'f000027', 'f000029', 'f000032', 'f000035', 'f000038', 'f000041', 'f000044', 'f000046', 'f000048', 'f000049', 'f000050', 'f000051', 'f000052', 'f000053', 'f000054', 'f000055', 'f000057', 'f000058', 'f000059', 'f000060', 'f000061', 'f000062', 'f000063', 'f000064', 'f000065', 'f000066', 'f000067', 'f000068', 'f000069', 'f000070', 'f000071', 'f000072', 'f000073', 'f000074', 'f000075', 'f000076', 'f000077', 'f000078', 'f000079', 'f000080', 'f000081', 'f000082', 'f000083', 'f000084', 'f000085', 'f000086', 'f000087', 'f000089', 'f000090', 'f000091', 'f000092', 'f000093', 'f000094', 'f000095', 'f000096', 'f000097', 'f000098', 'f000099', 'f000100', 'f000101', 'f000102', 'f000103']\n"
     ]
    }
   ],
   "source": [
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing dataframe\n",
    "\n",
    "Resize dataframe after features being selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape (238845, 104)\n",
      "Feature selection shape (238845, 70)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original shape\", df.shape)\n",
    "df = df[cols + [\"class\"]]\n",
    "X = df[cols]\n",
    "print(\"Feature selection shape\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing \n",
    "\n",
    "* [scikit train_test_split][train_test_split]: Split arrays or matrices into random train and test subsets.\n",
    "\n",
    "[train_test_split]:https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting train and test\n",
    "\n",
    "Split dataframe to create train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing \n",
    "\n",
    "* [scikit LogisticRegression][LogisticRegression]: Logistic Regression classifier.\n",
    "* [scikit RandomForestClassifier][RandomForestClassifier]: A random forest classifier.\n",
    "* [lightgbm LGBMClassifier][LGBMClassifier]: Construct a gradient boosting model.\n",
    "* [scikit RandomizedSearchCV][RandomizedSearchCV]: Randomized search on hyper parameters.\n",
    "* [numpy][numpy]: NumPy is the fundamental package for scientific computing with Python.\n",
    "* [scipy randint][randint]: A uniform discrete random variable.\n",
    "* [scipy uniform][uniform]: A uniform continuous random variable.\n",
    "\n",
    "[LogisticRegression]:https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "[RandomForestClassifier]:https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "[LGBMClassifier]:https://lightgbm.readthedocs.io/en/latest/Python-API.html\n",
    "[RandomizedSearchCV]:https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "[numpy]:https://www.numpy.org/\n",
    "[randint]:https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.randint.html\n",
    "[uniform]:https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "Tune parameters of Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/romulo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param: {'solver': 'liblinear', 'penalty': 'l1', 'n_jobs': 8, 'C': 5}\n",
      "Best score 0.8980091691264209\n"
     ]
    }
   ],
   "source": [
    "parameters = {\"penalty\":[\"l1\", \"l2\"],\n",
    "              \"C\":[0.1, 0.5, 1, 5, 10, 50, 100, 1000, 10000],\n",
    "              \"solver\":[\"liblinear\", \"saga\"],\n",
    "              \"n_jobs\":[8]}\n",
    "\n",
    "lg = RandomizedSearchCV(LogisticRegression(), parameters, cv = 5)\n",
    "lg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best param:\", lg.best_params_)\n",
    "print(\"Best score\", lg.best_score_)\n",
    "\n",
    "lg = lg.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param: {'n_jobs': 8, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 50, 'bootstrap': False}\n",
      "Best score 0.931111180891373\n"
     ]
    }
   ],
   "source": [
    "parameters = {'bootstrap': [True, False], \n",
    "              'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "              'n_jobs':[8]}\n",
    "\n",
    "rf = RandomizedSearchCV(RandomForestClassifier(), parameters, cv = 5)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best param:\", rf.best_params_)\n",
    "print(\"Best score\", rf.best_score_)\n",
    "\n",
    "rf = rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param: {'colsample_bytree': 0.86870234448516, 'min_child_samples': 341, 'min_child_weight': 0.01, 'n_jobs': 8, 'num_leaves': 26, 'reg_alpha': 2, 'reg_lambda': 100, 'subsample': 0.25372980851750904}\n",
      "Best score 0.9163369549289287\n"
     ]
    }
   ],
   "source": [
    "parameters ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n",
    "             'n_jobs':[8]}\n",
    "\n",
    "gb = RandomizedSearchCV(LGBMClassifier(), parameters, cv = 5)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best param:\", gb.best_params_)\n",
    "print(\"Best score\", gb.best_score_)\n",
    "\n",
    "gb = gb.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification score\n",
    "\n",
    "Calculate the score of the classification for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LG: 0.8984487847767381\n",
      "RF: 0.9363813351755322\n",
      "GB: 0.9166404990684335\n"
     ]
    }
   ],
   "source": [
    "print(\"LG: {}\".format(lg.score(X_test, y_test)))\n",
    "print(\"RF: {}\".format(rf.score(X_test, y_test)))\n",
    "print(\"GB: {}\".format(gb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing \n",
    "\n",
    "* [scikit VotingClassifier][VotingClassifier]: Soft Voting/Majority Rule classifier for unfitted estimators.\n",
    "* [scikit BaggingClassifier][BaggingClassifier]: A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions to form a final prediction.\n",
    "* [vecstack StackingTransformer][StackingTransformer]: Stacking (stacked generalization) is a machine learning ensembling technique.\n",
    "* [scikit accuracy_score][accuracy_score]: In multilabel classification, this function computes subset accuracy\n",
    "\n",
    "[VotingClassifier]:https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "[BaggingClassifier]:https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "[StackingTransformer]:https://github.com/vecxoz/vecstack\n",
    "[accuracy_score]:https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from vecstack import StackingTransformer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating ensemble classifier\n",
    "\n",
    "Create an ensemble classifier based on Logistic Regression, Random Forest and Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging Classifier\n",
    "\n",
    "Bagging classifier based on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BaggingClassifier(lg, n_jobs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier\n",
    "\n",
    "Voting classifier based on Bagging classifier (Logistic Regression), Random Forest and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators=[(\"BC\", bc), (\"RF\", rf), (\"GB\", gb)]\n",
    "ensemble = VotingClassifier(estimators, voting=\"soft\", n_jobs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking Classifier\n",
    "\n",
    "Stacking Classifier based on Bagging classifier (Logistic Regression), Random Forest and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [accuracy_score]\n",
      "variant:      [A]\n",
      "n_estimators: [3]\n",
      "\n",
      "estimator  0: [BC: BaggingClassifier]\n",
      "    fold  0:  [0.90066780]\n",
      "    fold  1:  [0.89692060]\n",
      "    fold  2:  [0.89782076]\n",
      "    fold  3:  [0.89606230]\n",
      "    ----\n",
      "    MEAN:     [0.89786786] + [0.00173199]\n",
      "\n",
      "estimator  1: [RF: RandomForestClassifier]\n",
      "    fold  0:  [0.93202705]\n",
      "    fold  1:  [0.92999644]\n",
      "    fold  2:  [0.93162930]\n",
      "    fold  3:  [0.92712847]\n",
      "    ----\n",
      "    MEAN:     [0.93019531] + [0.00192721]\n",
      "\n",
      "estimator  2: [GB: LGBMClassifier]\n",
      "    fold  0:  [0.91940380]\n",
      "    fold  1:  [0.91425401]\n",
      "    fold  2:  [0.91804308]\n",
      "    fold  3:  [0.91285143]\n",
      "    ----\n",
      "    MEAN:     [0.91613808] + [0.00267608]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stack = StackingTransformer(estimators, regression=False, verbose=2)\n",
    "stack = stack.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data\n",
    "\n",
    "Transform dataframe to stack format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set was detected.\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [BC: BaggingClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  1: [RF: RandomForestClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  2: [GB: LGBMClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [BC: BaggingClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  1: [RF: RandomForestClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  2: [GB: LGBMClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "S_train = stack.transform(X_train)\n",
    "S_test = stack.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting model\n",
    "\n",
    "Fit new model using training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('BC', BaggingClassifier(base_estimator=LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=8,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, wa...t=True,\n",
       "        subsample=0.25372980851750904, subsample_for_bin=200000,\n",
       "        subsample_freq=0))],\n",
       "         flatten_transform=None, n_jobs=8, voting='soft', weights=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.fit(S_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification score\n",
    "\n",
    "Calculate the score of the classification using test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9329690803659277"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.score(S_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing \n",
    "\n",
    "* [scikit joblib][joblib]: Joblib is a set of tools to provide lightweight pipelining in Python.\n",
    "\n",
    "[joblib]:https://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models\n",
    "\n",
    "Save models to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ensemble_speech.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(stack, \"stack_speech.pkl\")\n",
    "joblib.dump(ensemble, \"ensemble_speech.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_test']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib.dump(X_train, \"X_train\")\n",
    "#joblib.dump(X_test, \"X_test\")\n",
    "#joblib.dump(y_train, \"y_train\")\n",
    "#joblib.dump(y_test, \"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gb']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib.dump(lg, \"lg\")\n",
    "#joblib.dump(rf, \"rf\")\n",
    "#joblib.dump(gb, \"gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train = joblib.load(\"X_train\")\n",
    "#X_test = joblib.load(\"X_test\")\n",
    "#y_train = joblib.load(\"y_train\")\n",
    "#y_test = joblib.load(\"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lg = joblib.load(\"lg\")\n",
    "#rf = joblib.load(\"rf\")\n",
    "#gb = joblib.load(\"gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
